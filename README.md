# Research Task 08: LLM Bias Detection in Data Narratives

## 1. Project Overview

This project is a controlled experiment designed to detect systematic biases in data narratives generated by Large Language Models (LLMs). The core objective is to analyze whether minor variations in prompt framing, contextual information (proxy bias), or confirmation priming affect the LLM's interpretation of identical team-level sports statistics.

This research builds upon previous work involving LLM data narrative generation and ethical considerations.

---

## 2. Experimental Design (Phase 1: Completed)

The foundational experimental setup has been completed, resulting in a structured set of prompts ready for data collection.

### 2.1. Dataset and Sanitization

* **Dataset:** Team-level match statistics from the Premier League season (used in Task 5).
* **Sanitization (CRITICAL):** All actual team names (PII) were sanitized using anonymous identifiers (e.g., "Team Alpha," "Team Rho") to ensure ethical compliance. The raw data file (`Premier_League.csv`) is excluded via `.gitignore`.

### 2.2. Testable Hypotheses (H1, H2, H3)

Three primary hypotheses were designed to test different forms of bias:

| Hypothesis | Bias Type Tested | Prompt Variation |
| :--- | :--- | :--- |
| **H1** | Framing Effects | Negative Framing ("failures") vs. Positive Framing ("potential"). |
| **H2** | Proxy/Structural Bias | Neutral context vs. Context emphasizing a team's **low market status** (proxy for protected characteristics). |
| **H3** | Confirmation/Selection Bias | Neutral question vs. Priming the LLM with a **statistically secondary conclusion** (e.g., defense was the key factor). |

### 2.3. Prompts

A total of **6 unique prompt variations** (3 hypotheses $\times$ 2 conditions) were generated and stored in `prompts/all_prompts.csv`.

---

## 3. Next Steps (Phase 2 & Phase 3)

The project will now move into the data collection and analysis phases using the **Gemini** LLM.

### 3.1. Phase 2: Data Collection (Week 2)

* **Tool:** The `run_experiment.py` script, integrated with the Gemini API, will be used to execute the queries.
* **Execution:** Each of the 6 prompt variants will be queried multiple times to collect raw LLM responses, aiming for **4 samples per prompt** to account for model temperature/randomness.
* **Logging:** All raw responses, model versions, and metadata will be logged into structured files within the **`results/`** directory.

### 3.2. Phase 3: Analysis (Week 3)

* **Analysis Focus:** Quantitative analysis (sentiment, recommendation type counting) and Qualitative analysis (hallucination and cherry-picking).
* **Validation:** LLM claims will be validated against the Ground Truth using `validate_claims.py` to check for contradictions against the actual data.
